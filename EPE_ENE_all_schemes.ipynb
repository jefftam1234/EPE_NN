{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy  # For deepcopy\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Helper Functions for Aggregation\n",
    "# -------------------------------\n",
    "\n",
    "def aggregate_data_nosort(X, y, group_size):\n",
    "    \"\"\"\n",
    "    Aggregate the data by grouping contiguous rows (without any pre-sorting)\n",
    "    into groups of size 'group_size' and taking the mean over each group.\n",
    "    Any remainder (if total samples is not divisible by group_size) is dropped.\n",
    "    \"\"\"\n",
    "    n_groups = X.shape[0] // group_size\n",
    "    if n_groups < 1:\n",
    "        return X, y\n",
    "    X_cut = X[:n_groups * group_size]\n",
    "    y_cut = y[:n_groups * group_size]\n",
    "    X_agg = X_cut.reshape(n_groups, group_size, X.shape[1]).mean(axis=1)\n",
    "    y_agg = y_cut.reshape(n_groups, group_size).mean(axis=1)\n",
    "    return X_agg, y_agg\n",
    "\n",
    "def aggregate_X(X, group_size):\n",
    "    \"\"\"\n",
    "    Aggregate only the features X by grouping rows in order into batches of size group_size.\n",
    "    Returns the aggregated X (with shape [n_groups, features]) and the number of groups.\n",
    "    \"\"\"\n",
    "    n_groups = X.shape[0] // group_size\n",
    "    if n_groups < 1:\n",
    "        return X, None\n",
    "    X_cut = X[:n_groups * group_size]\n",
    "    X_agg = X_cut.reshape(n_groups, group_size, X.shape[1]).mean(axis=1)\n",
    "    return X_agg, n_groups\n",
    "\n",
    "def replicate_aggregated_predictions(agg_preds, group_size, total):\n",
    "    \"\"\"\n",
    "    Given aggregated predictions (one per group) and the original group_size,\n",
    "    replicate each group prediction 'group_size' times to produce a vector of length total.\n",
    "    If there is any remainder (i.e. total is not divisible by group_size),\n",
    "    fill those with the last aggregated prediction.\n",
    "    \"\"\"\n",
    "    preds = np.repeat(agg_preds, group_size)\n",
    "    if preds.shape[0] < total:\n",
    "        remainder = total - preds.shape[0]\n",
    "        preds = np.concatenate([preds, np.full(remainder, agg_preds[-1])])\n",
    "    return preds\n",
    "\n",
    "def compute_group_size(t_frac, N, num_steps):\n",
    "    \"\"\"\n",
    "    Given the time fraction (t/num_steps), number of samples N, and total number of time steps,\n",
    "    determine the group size for aggregation.\n",
    "      - At t=0 (t_frac==0): group all N paths (one datapoint).\n",
    "      - At t=T (t_frac==1): no aggregation (group_size=1).\n",
    "      - For very early nonzero times (e.g. the first nonzero time step), force grouping into 2 batches.\n",
    "      - Otherwise, use an exponential rule.\n",
    "    \"\"\"\n",
    "    if t_frac == 0:\n",
    "        return N  # aggregate all paths into one datapoint\n",
    "    if t_frac >= 1:\n",
    "        return 1  # no aggregation\n",
    "    min_nonzero = 1 / num_steps\n",
    "    if t_frac <= min_nonzero + 1e-8:\n",
    "        return N // 2\n",
    "    candidate = 2 ** (math.floor(math.log2(1 / t_frac)) + 1)\n",
    "    return candidate\n",
    "\n",
    "# -------------------------------\n",
    "# Main Classes\n",
    "# -------------------------------\n",
    "\n",
    "class BlackScholesMC:\n",
    "    def __init__(self, S0, K, r, sigma, T, num_paths, num_steps, seed=None):\n",
    "        self.S0, self.K, self.r, self.sigma, self.T = S0, K, r, sigma, T\n",
    "        self.num_paths, self.num_steps = num_paths, num_steps\n",
    "        self.dt = T / num_steps\n",
    "        self.discount_factors = np.exp(-r * self.dt)\n",
    "        self.seed = seed\n",
    "\n",
    "    def simulate_paths(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        S = np.zeros((self.num_paths, self.num_steps + 1))\n",
    "        S[:, 0] = self.S0\n",
    "        for t in range(1, self.num_steps + 1):\n",
    "            Z = np.random.randn(self.num_paths)\n",
    "            S[:, t] = S[:, t - 1] * np.exp(\n",
    "                (self.r - 0.5 * self.sigma ** 2) * self.dt + self.sigma * np.sqrt(self.dt) * Z\n",
    "            )\n",
    "        return S\n",
    "\n",
    "    def payoff(self, S):\n",
    "        return np.maximum(S[:, -1] - self.K, 0)\n",
    "\n",
    "class PolynomialRegressor:\n",
    "    def __init__(self, degree=2):\n",
    "        self.degree = degree\n",
    "        self.poly = PolynomialFeatures(degree)\n",
    "        self.model = LinearRegression()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Use full feature vector (state and time)\n",
    "        X_poly = self.poly.fit_transform(X)\n",
    "        self.model.fit(X_poly, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_poly = self.poly.transform(X)\n",
    "        return self.model.predict(X_poly).squeeze()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# New Local Regression Regressor (LOESS)\n",
    "# -------------------------------\n",
    "\n",
    "class LocalRegressor:\n",
    "    def __init__(self, bandwidth=None):\n",
    "        self.bandwidth = bandwidth\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Use only the state variable (first column) for local regression.\n",
    "        self.X_train = X[:, 0]\n",
    "        self.y_train = y\n",
    "        if self.bandwidth is None:\n",
    "            # A simple heuristic: 10% of the range of S values.\n",
    "            self.bandwidth = 0.1 * (np.max(self.X_train) - np.min(self.X_train))\n",
    "\n",
    "    def predict(self, X):\n",
    "        S_test = X[:, 0]\n",
    "        y_pred = np.empty(len(S_test))\n",
    "        for i, s in enumerate(S_test):\n",
    "            # Gaussian kernel weights based on the distance in S.\n",
    "            weights = np.exp(-((s - self.X_train)**2) / (2 * self.bandwidth**2))\n",
    "            if weights.sum() == 0:\n",
    "                y_pred[i] = np.mean(self.y_train)\n",
    "            else:\n",
    "                y_pred[i] = np.sum(weights * self.y_train) / np.sum(weights)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=64, output_dim=1):\n",
    "        super().__init__()\n",
    "        # Added dropout and reduced hidden_dim for better regularization\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        ).to(device)\n",
    "        self.constant = False\n",
    "        self.num_steps = None  # to be set by ExposureCalculator later\n",
    "\n",
    "    def fit(self, X, y, epochs=2000, lr=0.005):\n",
    "        # Aggregation is used here for training.\n",
    "        # Expect X[:,1] to be the time fraction.\n",
    "        t_frac = X[0, 1]\n",
    "        N = X.shape[0]\n",
    "        num_steps = self.num_steps if self.num_steps is not None else 50\n",
    "        group_size = compute_group_size(t_frac, N, num_steps)\n",
    "        if group_size > 1:\n",
    "            idx = np.argsort(X[:, 0])\n",
    "            X_sorted = X[idx]\n",
    "            y_sorted = y[idx]\n",
    "            X_sorted, y_sorted = aggregate_data_nosort(X_sorted, y_sorted, group_size)\n",
    "            if X_sorted.shape[0] == 1:\n",
    "                self.constant = True\n",
    "                self.constant_value = y_sorted[0]\n",
    "                return\n",
    "        else:\n",
    "            X_sorted, y_sorted = X, y\n",
    "\n",
    "        self.constant = False\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_sorted)\n",
    "        y_scaled = np.log1p(y_sorted)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "        y_tensor = torch.FloatTensor(y_scaled).unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(self.model(X_tensor), y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def predict(self, X):\n",
    "        # In prediction, we do NOT use aggregation.\n",
    "        if self.constant:\n",
    "            return np.full((X.shape[0],), self.constant_value)\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        with torch.no_grad():\n",
    "            pred = self.model(torch.FloatTensor(X_scaled).to(device)).cpu().numpy()\n",
    "        return np.expm1(pred).squeeze()\n",
    "\n",
    "\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=2, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size, padding=kernel_size//2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size, padding=kernel_size//2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.conv3 = nn.Conv1d(64, 32, kernel_size, padding=kernel_size//2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(32, 1)  # Applied on each element in the sequence.\n",
    "        self.to(device)\n",
    "        self.constant = False\n",
    "        self.num_steps = None  # to be set externally\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, seq_length)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        # Permute to (batch_size, seq_length, features) and apply fc to each time step.\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out = self.fc(x)  # shape: (batch_size, seq_length, 1)\n",
    "        return out\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, lr=0.005):\n",
    "        if np.std(X[:, 0]) < 1e-8:\n",
    "            self.constant = True\n",
    "            self.constant_value = np.mean(y)\n",
    "            return\n",
    "\n",
    "        # --- Aggregation Preprocessing for CNN ---\n",
    "        # For CNN we aggregate without pre-sorting the original data.\n",
    "        t_frac = X[0, 1]\n",
    "        N = X.shape[0]\n",
    "        num_steps = self.num_steps if self.num_steps is not None else 50\n",
    "        group_size = compute_group_size(t_frac, N, num_steps)\n",
    "        if group_size > 1:\n",
    "            X_agg, y_agg = aggregate_data_nosort(X, y, group_size)\n",
    "            # Now sort the aggregated data by the aggregated stock price (first column)\n",
    "            sort_idx = np.argsort(X_agg[:, 0])\n",
    "            X_sorted = X_agg[sort_idx]\n",
    "            y_sorted = y_agg[sort_idx]\n",
    "            if X_sorted.shape[0] == 1:\n",
    "                self.constant = True\n",
    "                self.constant_value = y_sorted[0]\n",
    "                return\n",
    "        else:\n",
    "            X_sorted, y_sorted = X, y\n",
    "\n",
    "        self.constant = False\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X_sorted)  # shape: (n_samples, 2)\n",
    "        # Reshape X to (batch_size=1, channels=2, seq_length=n_samples)\n",
    "        X_tensor = torch.FloatTensor(X_scaled.T).unsqueeze(0).to(device)\n",
    "        y_transformed = np.log1p(y_sorted)\n",
    "        y_tensor = torch.FloatTensor(y_transformed).unsqueeze(0).unsqueeze(2).to(device)\n",
    "\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        criterion = nn.MSELoss()\n",
    "        self.train()\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(X_tensor)\n",
    "            loss = criterion(output, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        self.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.constant:\n",
    "            return np.full((X.shape[0],), self.constant_value)\n",
    "        t_frac = X[0, 1]\n",
    "        N = X.shape[0]\n",
    "        group_size = compute_group_size(t_frac, N, self.num_steps if self.num_steps is not None else 50)\n",
    "        if group_size > 1:\n",
    "            # Aggregate test data (without pre-sorting)\n",
    "            X_agg, _ = aggregate_X(X, group_size)\n",
    "            # Now sort the aggregated data by aggregated state (first column)\n",
    "            sort_idx = np.argsort(X_agg[:, 0])\n",
    "            X_agg_sorted = X_agg[sort_idx]\n",
    "            X_scaled = self.scaler.transform(X_agg_sorted)\n",
    "            X_tensor = torch.FloatTensor(X_scaled.T).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = self.forward(X_tensor)\n",
    "            agg_preds_sorted = np.expm1(output.cpu().numpy().squeeze())\n",
    "            # Unsort the aggregated predictions to match original grouping order.\n",
    "            unsort_idx = np.argsort(sort_idx)\n",
    "            agg_preds = agg_preds_sorted[unsort_idx]\n",
    "            y_pred = replicate_aggregated_predictions(agg_preds, group_size, X.shape[0])\n",
    "            return y_pred\n",
    "        else:\n",
    "            # If no aggregation is used, sort test data by state variable for prediction.\n",
    "            sorted_idx = np.argsort(X[:, 0])\n",
    "            X_sorted = X[sorted_idx]\n",
    "            X_scaled = self.scaler.transform(X_sorted)\n",
    "            X_tensor = torch.FloatTensor(X_scaled.T).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = self.forward(X_tensor)\n",
    "            y_pred_sorted = np.expm1(output.cpu().numpy().squeeze())\n",
    "            y_pred = np.zeros_like(y_pred_sorted)\n",
    "            y_pred[sorted_idx] = y_pred_sorted\n",
    "            return y_pred\n",
    "\n",
    "\n",
    "\n",
    "class StrongRegNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A feed-forward NN regressor that blends a constant baseline with a learned mapping.\n",
    "\n",
    "    The final output is computed as:\n",
    "\n",
    "        output = (1 - α(t)) * baseline + α(t) * f_NN(x)\n",
    "\n",
    "    where α(t) = sigmoid((t - threshold) / scale) is the mixing weight computed from the time feature,\n",
    "    baseline is the mean target on training data (enforced nonnegative),\n",
    "    and f_NN is a feed-forward network (with Softplus final activation to guarantee nonnegative outputs).\n",
    "\n",
    "    At early times (when t is very small) α(t) is near 0, so the output is almost constant,\n",
    "    and as time increases, the network’s prediction takes over.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=128, output_dim=1, threshold=0.2, scale=0.1, dropout=0.2, use_softplus=True):\n",
    "        super().__init__()\n",
    "        self.threshold = threshold\n",
    "        self.scale = scale\n",
    "        self.use_softplus = use_softplus\n",
    "\n",
    "        # Define the base network.\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        ).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "        # Choose final activation based on flag.\n",
    "        self.final_activation = nn.Softplus() if self.use_softplus else nn.Identity()\n",
    "\n",
    "        self.constant = False\n",
    "        self.baseline = None\n",
    "        self.scaler = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: tensor with columns [state S, time fraction t]\n",
    "        t = x[:, 1:2]  # Extract time feature.\n",
    "        # Mixing weight: near 0 at early times and near 1 at later times.\n",
    "        alpha = torch.sigmoid((t - self.threshold) / self.scale)\n",
    "        # Network prediction with chosen final activation.\n",
    "        f = self.final_activation(self.network(x))\n",
    "        # Baseline is a constant computed during training.\n",
    "        baseline = self.baseline.expand_as(alpha)\n",
    "        # Blend constant and learned output.\n",
    "        output = (1 - alpha) * baseline + alpha * f\n",
    "        return output\n",
    "\n",
    "    def fit(self, X, y, epochs=2000, lr=0.005):\n",
    "        \"\"\"\n",
    "        X: numpy array of shape (N,2), with columns: [state S, time fraction t/num_steps]\n",
    "        y: numpy array of shape (N,) containing the target exposure.\n",
    "        \"\"\"\n",
    "        # If the state variable is nearly constant (e.g. at t=0), simply store the constant target.\n",
    "        if np.std(X[:, 0]) < 1e-8:\n",
    "            self.constant = True\n",
    "            self.baseline = torch.tensor(np.mean(y), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            return\n",
    "\n",
    "        self.constant = False\n",
    "        # Set the baseline as the (nonnegative) mean of the target exposures.\n",
    "        mean_y = np.mean(y)\n",
    "        self.baseline = torch.tensor(mean_y, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Scale the features.\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "        # We work directly with y (which should be nonnegative) and use MSE loss.\n",
    "        y_tensor = torch.FloatTensor(y).unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        self.train()\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(X_tensor)\n",
    "            loss = criterion(output, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        self.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X: numpy array of shape (N,2). Returns predictions as a numpy array of shape (N,).\n",
    "        \"\"\"\n",
    "        if self.constant:\n",
    "            return np.full((X.shape[0],), self.baseline.item())\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = self.forward(X_tensor).cpu().numpy()\n",
    "        return pred.squeeze()\n",
    "\n",
    "\n",
    "class ExposureCalculator:\n",
    "    def __init__(self, mc_engine_1st, mc_engine_2nd, regressor):\n",
    "        self.mc_engine_1st = mc_engine_1st\n",
    "        self.mc_engine_2nd = mc_engine_2nd\n",
    "        self.regressor = regressor\n",
    "        self.regressors = {}\n",
    "\n",
    "    def first_pass(self):\n",
    "        S = self.mc_engine_1st.simulate_paths()\n",
    "        option_payoff = self.mc_engine_1st.payoff(S)\n",
    "        pathwise_exposure = np.zeros((self.mc_engine_1st.num_paths, self.mc_engine_1st.num_steps + 1))\n",
    "        pathwise_exposure[:, -1] = option_payoff\n",
    "        for t in range(self.mc_engine_1st.num_steps - 1, -1, -1):\n",
    "            pathwise_exposure[:, t] = pathwise_exposure[:, t + 1] * self.mc_engine_1st.discount_factors\n",
    "            t_feature = np.full((self.mc_engine_1st.num_paths, 1), t / self.mc_engine_1st.num_steps)\n",
    "            X_train = np.hstack((S[:, t].reshape(-1, 1), t_feature))\n",
    "            # Use deepcopy to preserve regressor settings.\n",
    "            self.regressors[t] = copy.deepcopy(self.regressor)\n",
    "            if isinstance(self.regressors[t], (FullyConnectedNN, CNNRegressor)):\n",
    "                self.regressors[t].num_steps = self.mc_engine_1st.num_steps\n",
    "            self.regressors[t].fit(X_train, pathwise_exposure[:, t])\n",
    "\n",
    "    def second_pass(self):\n",
    "        if not self.regressors:\n",
    "            raise ValueError(\"first_pass() must be run before second_pass()!\")\n",
    "        S = self.mc_engine_2nd.simulate_paths()\n",
    "        estimated_exposure = np.zeros((self.mc_engine_2nd.num_paths, self.mc_engine_2nd.num_steps + 1))\n",
    "        for t in range(self.mc_engine_2nd.num_steps):\n",
    "            t_feature = np.full((self.mc_engine_2nd.num_paths, 1), t / self.mc_engine_2nd.num_steps)\n",
    "            X_test = np.hstack((S[:, t].reshape(-1, 1), t_feature))\n",
    "            if t not in self.regressors:\n",
    "                raise KeyError(f\"Regressor for t={t} not found. Make sure first_pass() was executed.\")\n",
    "            estimated_exposure[:, t] = self.regressors[t].predict(X_test).squeeze()\n",
    "        estimated_exposure[:, self.mc_engine_2nd.num_steps] = self.mc_engine_2nd.payoff(S)\n",
    "        EPE = np.mean(np.maximum(estimated_exposure, 0), axis=0)\n",
    "        ENE = np.mean(np.minimum(estimated_exposure, 0), axis=0)\n",
    "        EE = np.mean(estimated_exposure, axis=0)\n",
    "        return EPE, ENE, EE\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    S0, K, r, sigma, T = 140, 140, 0.02, 0.2, 5\n",
    "    # r = 0.0 # Risk-free rate is set to 0 for simplicity.\n",
    "    num_paths_1st, num_paths_2nd, num_steps = 5000, 20000, 50\n",
    "    seed_1st, seed_2nd = 42, 99\n",
    "\n",
    "    mc_engine_1st = BlackScholesMC(S0, K, r, sigma, T, num_paths_1st, num_steps, seed_1st)\n",
    "    mc_engine_2nd = BlackScholesMC(S0, K, r, sigma, T, num_paths_2nd, num_steps, seed_2nd)\n",
    "\n",
    "    regressors = {\n",
    "        \"Polynomial (Degree 2)\": PolynomialRegressor(degree=2),\n",
    "        \"Polynomial (Degree 3)\": PolynomialRegressor(degree=3),\n",
    "        # \"Local Regression\": LocalRegressor(),  # (LocalRegression remains as before.)\n",
    "        # \"Fully Connected NN\": FullyConnectedNN(),\n",
    "        # \"CNN Regressor\": CNNRegressor(),  # Uncomment to use the CNN-based regressor.\n",
    "        # \"StrongRegNN_zerofloor\" : StrongRegNN(use_softplus=True),\n",
    "        \"StrongRegNN\" : StrongRegNN(use_softplus=False)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, reg in regressors.items():\n",
    "        print(f\"Processing {name} on {device}...\")\n",
    "        exposure_calc = ExposureCalculator(mc_engine_1st, mc_engine_2nd, reg)\n",
    "        exposure_calc.first_pass()  # Must call first_pass() before second_pass()\n",
    "        results[name] = exposure_calc.second_pass()"
   ],
   "id": "82a0a83ff614a9d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    results = {k: v for k, v in results.items() if k not in [\"CNN Regressor\", \"StrongRegNN_zerofloor\"] }\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for name, (EPE, ENE, EE) in results.items():\n",
    "        plt.plot(np.linspace(0, T, num_steps + 1), EPE, label=f\"EPE - {name}\", linewidth=2)\n",
    "        plt.plot(np.linspace(0, T, num_steps + 1), ENE, linestyle=\"--\", label=f\"ENE - {name}\", linewidth=2)\n",
    "\n",
    "    plt.title(\"EPE, ENE with different regressors\")\n",
    "    plt.xlabel(\"Time (years)\")\n",
    "    plt.ylabel(\"Exposure\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "14b3e44d29cea23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "    plt.figure(figsize=(10, 5))\n",
    "    for name, (EPE, ENE, EE) in results.items():\n",
    "        plt.plot(np.linspace(0, T, num_steps + 1), EE, label=f\"EE - {name}\", linewidth=2)\n",
    "\n",
    "    plt.title(\"EE with different regressors\")\n",
    "    plt.xlabel(\"Time (years)\")\n",
    "    plt.ylabel(\"Exposure\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ],
   "id": "93a668ac93ed61a7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

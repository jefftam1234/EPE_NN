{
 "cells": [
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-02-24T10:30:54.518067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy  # For deepcopy\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Helper Functions for Aggregation\n",
    "# -------------------------------\n",
    "\n",
    "def aggregate_data_nosort(X, y, group_size):\n",
    "    \"\"\"\n",
    "    Aggregate the data by grouping contiguous rows (without any pre-sorting)\n",
    "    into groups of size 'group_size' and taking the mean over each group.\n",
    "    Any remainder (if total samples is not divisible by group_size) is dropped.\n",
    "    \"\"\"\n",
    "    n_groups = X.shape[0] // group_size\n",
    "    if n_groups < 1:\n",
    "        return X, y\n",
    "    X_cut = X[:n_groups * group_size]\n",
    "    y_cut = y[:n_groups * group_size]\n",
    "    X_agg = X_cut.reshape(n_groups, group_size, X.shape[1]).mean(axis=1)\n",
    "    y_agg = y_cut.reshape(n_groups, group_size).mean(axis=1)\n",
    "    return X_agg, y_agg\n",
    "\n",
    "def aggregate_X(X, group_size):\n",
    "    \"\"\"\n",
    "    Aggregate only the features X by grouping rows in order into batches of size group_size.\n",
    "    Returns the aggregated X (with shape [n_groups, features]) and the number of groups.\n",
    "    \"\"\"\n",
    "    n_groups = X.shape[0] // group_size\n",
    "    if n_groups < 1:\n",
    "        return X, None\n",
    "    X_cut = X[:n_groups * group_size]\n",
    "    X_agg = X_cut.reshape(n_groups, group_size, X.shape[1]).mean(axis=1)\n",
    "    return X_agg, n_groups\n",
    "\n",
    "def replicate_aggregated_predictions(agg_preds, group_size, total):\n",
    "    \"\"\"\n",
    "    Given aggregated predictions (one per group) and the original group_size,\n",
    "    replicate each group prediction 'group_size' times to produce a vector of length total.\n",
    "    If there is any remainder (i.e. total is not divisible by group_size),\n",
    "    fill those with the last aggregated prediction.\n",
    "    \"\"\"\n",
    "    preds = np.repeat(agg_preds, group_size)\n",
    "    if preds.shape[0] < total:\n",
    "        remainder = total - preds.shape[0]\n",
    "        preds = np.concatenate([preds, np.full(remainder, agg_preds[-1])])\n",
    "    return preds\n",
    "\n",
    "def compute_group_size(t_frac, N, num_steps):\n",
    "    \"\"\"\n",
    "    Given the time fraction (t/num_steps), number of samples N, and total number of time steps,\n",
    "    determine the group size for aggregation.\n",
    "      - At t=0 (t_frac==0): group all N paths (one datapoint).\n",
    "      - At t=T (t_frac==1): no aggregation (group_size=1).\n",
    "      - For very early nonzero times (e.g. the first nonzero time step), force grouping into 2 batches.\n",
    "      - Otherwise, use an exponential rule.\n",
    "    \"\"\"\n",
    "    if t_frac == 0:\n",
    "        return N  # aggregate all paths into one datapoint\n",
    "    if t_frac >= 1:\n",
    "        return 1  # no aggregation\n",
    "    min_nonzero = 1 / num_steps\n",
    "    if t_frac <= min_nonzero + 1e-8:\n",
    "        return N // 2\n",
    "    candidate = 2 ** (math.floor(math.log2(1 / t_frac)) + 1)\n",
    "    return candidate\n",
    "\n",
    "# -------------------------------\n",
    "# Main Classes\n",
    "# -------------------------------\n",
    "\n",
    "class BlackScholesMC:\n",
    "    def __init__(self, S0, K, r, sigma, T, num_paths, num_steps, seed=None):\n",
    "        self.S0, self.K, self.r, self.sigma, self.T = S0, K, r, sigma, T\n",
    "        self.num_paths, self.num_steps = num_paths, num_steps\n",
    "        self.dt = T / num_steps\n",
    "        self.discount_factors = np.exp(-r * self.dt)\n",
    "        self.seed = seed\n",
    "\n",
    "    def simulate_paths(self):\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "        S = np.zeros((self.num_paths, self.num_steps + 1))\n",
    "        S[:, 0] = self.S0\n",
    "        for t in range(1, self.num_steps + 1):\n",
    "            Z = np.random.randn(self.num_paths)\n",
    "            S[:, t] = S[:, t - 1] * np.exp(\n",
    "                (self.r - 0.5 * self.sigma ** 2) * self.dt + self.sigma * np.sqrt(self.dt) * Z\n",
    "            )\n",
    "        return S\n",
    "\n",
    "    def payoff(self, S):\n",
    "        return np.maximum(S[:, -1] - self.K, 0)\n",
    "\n",
    "class PolynomialRegressor:\n",
    "    def __init__(self, degree=2):\n",
    "        self.degree = degree\n",
    "        self.poly = PolynomialFeatures(degree)\n",
    "        self.model = LinearRegression()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Use full feature vector (state and time)\n",
    "        X_poly = self.poly.fit_transform(X)\n",
    "        self.model.fit(X_poly, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_poly = self.poly.transform(X)\n",
    "        return self.model.predict(X_poly).squeeze()\n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, input_dim=2, hidden_dim=128, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        ).to(device)\n",
    "        self.constant = False\n",
    "        self.num_steps = None  # to be set by ExposureCalculator\n",
    "\n",
    "    def fit(self, X, y, epochs=2000, lr=0.005):\n",
    "        # If the state variable (first column) is nearly constant, simply store the mean target.\n",
    "        if np.std(X[:, 0]) < 1e-8:\n",
    "            self.constant = True\n",
    "            self.constant_value = np.mean(y)\n",
    "            return\n",
    "\n",
    "        # --- Aggregation Preprocessing ---\n",
    "        # Expect X[:,1] to be the time fraction.\n",
    "        t_frac = X[0, 1]\n",
    "        N = X.shape[0]\n",
    "        num_steps = self.num_steps if self.num_steps is not None else 50\n",
    "        group_size = compute_group_size(t_frac, N, num_steps)\n",
    "        if group_size > 1:\n",
    "            # For FullyConnectedNN we use aggregation after sorting.\n",
    "            idx = np.argsort(X[:, 0])\n",
    "            X_sorted = X[idx]\n",
    "            y_sorted = y[idx]\n",
    "            X_sorted, y_sorted = aggregate_data_nosort(X_sorted, y_sorted, group_size)\n",
    "            if X_sorted.shape[0] == 1:\n",
    "                self.constant = True\n",
    "                self.constant_value = y_sorted[0]\n",
    "                return\n",
    "        else:\n",
    "            X_sorted, y_sorted = X, y\n",
    "\n",
    "        self.constant = False\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_sorted)\n",
    "        y_scaled = np.log1p(y_sorted)\n",
    "\n",
    "        X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "        y_tensor = torch.FloatTensor(y_scaled).unsqueeze(1).to(device)\n",
    "\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(self.model(X_tensor), y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.constant:\n",
    "            return np.full((X.shape[0],), self.constant_value)\n",
    "        # Use the same aggregation logic as in training.\n",
    "        t_frac = X[0, 1]\n",
    "        N = X.shape[0]\n",
    "        group_size = compute_group_size(t_frac, N, self.num_steps if self.num_steps is not None else 50)\n",
    "        if group_size > 1:\n",
    "            X_agg, _ = aggregate_X(X, group_size)\n",
    "            X_scaled = self.scaler.transform(X_agg)\n",
    "            X_tensor = torch.FloatTensor(X_scaled).to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = self.model(X_tensor).cpu().numpy()\n",
    "            agg_preds = np.expm1(pred).squeeze()\n",
    "            y_pred = replicate_aggregated_predictions(agg_preds, group_size, X.shape[0])\n",
    "            return y_pred\n",
    "        else:\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            with torch.no_grad():\n",
    "                pred = self.model(torch.FloatTensor(X_scaled).to(device)).cpu().numpy()\n",
    "            return np.expm1(pred).squeeze()\n",
    "\n",
    "class LocalRegression:\n",
    "    \"\"\"\n",
    "    A LOESS-style regressor using local weighted linear regression.\n",
    "    For each query point, a subset of nearest neighbors (by Euclidean distance in feature space)\n",
    "    is selected (controlled by frac). A local linear model is then fit with weights from a Gaussian kernel.\n",
    "    Optional robustifying iterations (it) reweight large residuals.\n",
    "    \"\"\"\n",
    "    def __init__(self, frac=0.3, it=3):\n",
    "        self.frac = frac  # Fraction of training data to use for each local fit\n",
    "        self.it = it      # Number of robustifying iterations\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Store the training data\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        n_train = self.X_train.shape[0]\n",
    "        n_neighbors = max(int(self.frac * n_train), 2)  # at least 2 neighbors\n",
    "        for i, x in enumerate(X):\n",
    "            # Compute Euclidean distances between x and all training points\n",
    "            distances = np.linalg.norm(self.X_train - x, axis=1)\n",
    "            idx = np.argsort(distances)[:n_neighbors]\n",
    "            X_neighbors = self.X_train[idx]\n",
    "            y_neighbors = self.y_train[idx]\n",
    "            # Determine maximum distance among neighbors to scale the kernel\n",
    "            d_max = distances[idx[-1]]\n",
    "            if d_max == 0:\n",
    "                kernel_weights = np.ones(n_neighbors, dtype=float)\n",
    "            else:\n",
    "                kernel_weights = np.exp(- (distances[idx] / d_max)**2)\n",
    "            # Prepare design matrix with intercept and features\n",
    "            X_design = np.hstack((np.ones((n_neighbors, 1)), X_neighbors))\n",
    "            # Initialize robust weights as ones\n",
    "            robust_weights = np.ones(n_neighbors, dtype=float)\n",
    "            # Iteratively reweight for robustness\n",
    "            for _ in range(self.it):\n",
    "                combined_weights = kernel_weights * robust_weights\n",
    "                W = np.diag(combined_weights)\n",
    "                try:\n",
    "                    beta = np.linalg.inv(X_design.T @ W @ X_design) @ (X_design.T @ W @ y_neighbors)\n",
    "                except np.linalg.LinAlgError:\n",
    "                    beta = np.zeros(X_design.shape[1])\n",
    "                # Compute residuals and update robust weights using bisquare function\n",
    "                y_fit = X_design @ beta\n",
    "                residuals = np.abs(y_neighbors - y_fit)\n",
    "                median_res = np.median(residuals)\n",
    "                if median_res < 1e-8:\n",
    "                    break\n",
    "                u = residuals / (6 * median_res)\n",
    "                robust_weights = np.where(np.abs(u) < 1, (1 - u**2)**2, 0)\n",
    "            # Predict at x using the final local linear model\n",
    "            x_design = np.hstack(([1], x))\n",
    "            predictions[i] = x_design.dot(beta)\n",
    "        return predictions\n",
    "\n",
    "class CNNRegressor(nn.Module):\n",
    "    def __init__(self, in_channels=2, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, 64, kernel_size, padding=kernel_size//2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size, padding=kernel_size//2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.conv3 = nn.Conv1d(64, 32, kernel_size, padding=kernel_size//2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(32, 1)  # Applied on each element in the sequence.\n",
    "        self.to(device)\n",
    "        self.constant = False\n",
    "        self.num_steps = None  # to be set externally\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels, seq_length)\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        # Permute to (batch_size, seq_length, features) and apply fc to each time step.\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out = self.fc(x)  # shape: (batch_size, seq_length, 1)\n",
    "        return out\n",
    "\n",
    "    def fit(self, X, y, epochs=1000, lr=0.005):\n",
    "        if np.std(X[:, 0]) < 1e-8:\n",
    "            self.constant = True\n",
    "            self.constant_value = np.mean(y)\n",
    "            return\n",
    "\n",
    "        # --- Aggregation Preprocessing for CNN ---\n",
    "        # For CNN we aggregate without pre-sorting the original data.\n",
    "        t_frac = X[0, 1]\n",
    "        N = X.shape[0]\n",
    "        num_steps = self.num_steps if self.num_steps is not None else 50\n",
    "        group_size = compute_group_size(t_frac, N, num_steps)\n",
    "        if group_size > 1:\n",
    "            X_agg, y_agg = aggregate_data_nosort(X, y, group_size)\n",
    "            # Now sort the aggregated data by the aggregated stock price (first column)\n",
    "            sort_idx = np.argsort(X_agg[:, 0])\n",
    "            X_sorted = X_agg[sort_idx]\n",
    "            y_sorted = y_agg[sort_idx]\n",
    "            if X_sorted.shape[0] == 1:\n",
    "                self.constant = True\n",
    "                self.constant_value = y_sorted[0]\n",
    "                return\n",
    "        else:\n",
    "            X_sorted, y_sorted = X, y\n",
    "\n",
    "        self.constant = False\n",
    "        self.scaler = StandardScaler()\n",
    "        X_scaled = self.scaler.fit_transform(X_sorted)  # shape: (n_samples, 2)\n",
    "        # Reshape X to (batch_size=1, channels=2, seq_length=n_samples)\n",
    "        X_tensor = torch.FloatTensor(X_scaled.T).unsqueeze(0).to(device)\n",
    "        y_transformed = np.log1p(y_sorted)\n",
    "        y_tensor = torch.FloatTensor(y_transformed).unsqueeze(0).unsqueeze(2).to(device)\n",
    "\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        criterion = nn.MSELoss()\n",
    "        self.train()\n",
    "        for _ in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(X_tensor)\n",
    "            loss = criterion(output, y_tensor)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        self.eval()\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.constant:\n",
    "            return np.full((X.shape[0],), self.constant_value)\n",
    "        t_frac = X[0, 1]\n",
    "        N = X.shape[0]\n",
    "        group_size = compute_group_size(t_frac, N, self.num_steps if self.num_steps is not None else 50)\n",
    "        if group_size > 1:\n",
    "            # Aggregate test data (without pre-sorting)\n",
    "            X_agg, _ = aggregate_X(X, group_size)\n",
    "            # Now sort the aggregated data by aggregated state (first column)\n",
    "            sort_idx = np.argsort(X_agg[:, 0])\n",
    "            X_agg_sorted = X_agg[sort_idx]\n",
    "            X_scaled = self.scaler.transform(X_agg_sorted)\n",
    "            X_tensor = torch.FloatTensor(X_scaled.T).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = self.forward(X_tensor)\n",
    "            agg_preds_sorted = np.expm1(output.cpu().numpy().squeeze())\n",
    "            # Unsort the aggregated predictions to match original grouping order.\n",
    "            unsort_idx = np.argsort(sort_idx)\n",
    "            agg_preds = agg_preds_sorted[unsort_idx]\n",
    "            y_pred = replicate_aggregated_predictions(agg_preds, group_size, X.shape[0])\n",
    "            return y_pred\n",
    "        else:\n",
    "            # If no aggregation is used, sort test data by state variable for prediction.\n",
    "            sorted_idx = np.argsort(X[:, 0])\n",
    "            X_sorted = X[sorted_idx]\n",
    "            X_scaled = self.scaler.transform(X_sorted)\n",
    "            X_tensor = torch.FloatTensor(X_scaled.T).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                output = self.forward(X_tensor)\n",
    "            y_pred_sorted = np.expm1(output.cpu().numpy().squeeze())\n",
    "            y_pred = np.zeros_like(y_pred_sorted)\n",
    "            y_pred[sorted_idx] = y_pred_sorted\n",
    "            return y_pred\n",
    "\n",
    "class ExposureCalculator:\n",
    "    def __init__(self, mc_engine_1st, mc_engine_2nd, regressor):\n",
    "        self.mc_engine_1st = mc_engine_1st\n",
    "        self.mc_engine_2nd = mc_engine_2nd\n",
    "        self.regressor = regressor\n",
    "        self.regressors = {}\n",
    "\n",
    "    def first_pass(self):\n",
    "        S = self.mc_engine_1st.simulate_paths()\n",
    "        option_payoff = self.mc_engine_1st.payoff(S)\n",
    "        pathwise_exposure = np.zeros((self.mc_engine_1st.num_paths, self.mc_engine_1st.num_steps + 1))\n",
    "        pathwise_exposure[:, -1] = option_payoff\n",
    "        for t in range(self.mc_engine_1st.num_steps - 1, -1, -1):\n",
    "            pathwise_exposure[:, t] = pathwise_exposure[:, t + 1] * self.mc_engine_1st.discount_factors\n",
    "            t_feature = np.full((self.mc_engine_1st.num_paths, 1), t / self.mc_engine_1st.num_steps)\n",
    "            X_train = np.hstack((S[:, t].reshape(-1, 1), t_feature))\n",
    "            # Use deepcopy to preserve regressor settings.\n",
    "            self.regressors[t] = copy.deepcopy(self.regressor)\n",
    "            if isinstance(self.regressors[t], (FullyConnectedNN, CNNRegressor)):\n",
    "                self.regressors[t].num_steps = self.mc_engine_1st.num_steps\n",
    "            self.regressors[t].fit(X_train, pathwise_exposure[:, t])\n",
    "\n",
    "    def second_pass(self):\n",
    "        if not self.regressors:\n",
    "            raise ValueError(\"first_pass() must be run before second_pass()!\")\n",
    "        S = self.mc_engine_2nd.simulate_paths()\n",
    "        estimated_exposure = np.zeros((self.mc_engine_2nd.num_paths, self.mc_engine_2nd.num_steps + 1))\n",
    "        for t in range(self.mc_engine_2nd.num_steps):\n",
    "            t_feature = np.full((self.mc_engine_2nd.num_paths, 1), t / self.mc_engine_2nd.num_steps)\n",
    "            X_test = np.hstack((S[:, t].reshape(-1, 1), t_feature))\n",
    "            if t not in self.regressors:\n",
    "                raise KeyError(f\"Regressor for t={t} not found. Make sure first_pass() was executed.\")\n",
    "            estimated_exposure[:, t] = self.regressors[t].predict(X_test).squeeze()\n",
    "        estimated_exposure[:, self.mc_engine_2nd.num_steps] = self.mc_engine_2nd.payoff(S)\n",
    "        EPE = np.mean(np.maximum(estimated_exposure, 0), axis=0)\n",
    "        ENE = np.mean(np.minimum(estimated_exposure, 0), axis=0)\n",
    "        return EPE, ENE\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    S0, K, r, sigma, T = 140, 140, 0.02, 0.2, 5\n",
    "    num_paths_1st, num_paths_2nd, num_steps = 5000, 20000, 50\n",
    "    seed_1st, seed_2nd = 42, 99\n",
    "\n",
    "    mc_engine_1st = BlackScholesMC(S0, K, r, sigma, T, num_paths_1st, num_steps, seed_1st)\n",
    "    mc_engine_2nd = BlackScholesMC(S0, K, r, sigma, T, num_paths_2nd, num_steps, seed_2nd)\n",
    "\n",
    "    regressors = {\n",
    "        \"Polynomial (Degree 2)\": PolynomialRegressor(degree=2),\n",
    "        \"Polynomial (Degree 3)\": PolynomialRegressor(degree=3),\n",
    "        \"Local Regression\": LocalRegression(),  # (LocalRegression remains as before.)\n",
    "        \"Fully Connected NN\": FullyConnectedNN(),\n",
    "        \"CNN Regressor\": CNNRegressor()  # Uncomment to use the CNN-based regressor.\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    for name, reg in regressors.items():\n",
    "        print(f\"Processing {name} on {device}...\")\n",
    "        exposure_calc = ExposureCalculator(mc_engine_1st, mc_engine_2nd, reg)\n",
    "        exposure_calc.first_pass()  # Must call first_pass() before second_pass()\n",
    "        results[name] = exposure_calc.second_pass()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for name, (EPE, ENE) in results.items():\n",
    "        plt.plot(np.linspace(0, T, num_steps + 1), EPE, label=f\"EPE - {name}\", linewidth=2)\n",
    "        plt.plot(np.linspace(0, T, num_steps + 1), ENE, linestyle=\"--\", label=f\"ENE - {name}\", linewidth=2)\n",
    "\n",
    "    plt.xlabel(\"Time (years)\")\n",
    "    plt.ylabel(\"Exposure\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "id": "82a0a83ff614a9d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processing Polynomial (Degree 2) on cuda...\n",
      "Processing Polynomial (Degree 3) on cuda...\n",
      "Processing Local Regression on cuda...\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dca71e5fcfe50a5c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
